{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fsspec in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (2022.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install fsspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gcsfs in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (2022.2.0)\n",
            "Requirement already satisfied: google-auth>=1.2 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from gcsfs) (2.6.0)\n",
            "Requirement already satisfied: decorator>4.1.2 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from gcsfs) (5.1.0)\n",
            "Requirement already satisfied: requests in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from gcsfs) (2.26.0)\n",
            "Requirement already satisfied: aiohttp<4 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from gcsfs) (3.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from gcsfs) (0.4.6)\n",
            "Requirement already satisfied: google-cloud-storage in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from gcsfs) (2.2.0)\n",
            "Requirement already satisfied: fsspec==2022.02.0 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from gcsfs) (2022.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from aiohttp<4->gcsfs) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from aiohttp<4->gcsfs) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from aiohttp<4->gcsfs) (2.0.8)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from aiohttp<4->gcsfs) (1.7.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from aiohttp<4->gcsfs) (1.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from aiohttp<4->gcsfs) (6.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from aiohttp<4->gcsfs) (21.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from google-auth>=1.2->gcsfs) (1.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from google-auth>=1.2->gcsfs) (5.0.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from google-auth>=1.2->gcsfs) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from google-cloud-storage->gcsfs) (2.3.2)\n",
            "Requirement already satisfied: protobuf in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from google-cloud-storage->gcsfs) (3.19.4)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from google-cloud-storage->gcsfs) (2.7.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from google-cloud-storage->gcsfs) (2.2.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from requests->gcsfs) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from requests->gcsfs) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from requests->gcsfs) (3.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.55.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\tienda\\desktop\\masteria\\tfm\\tfm-miax7\\myenv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install gcsfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sBdI_9X5D1ey"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tensorflow import keras\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def smape_loss(y_true, y_pred):\n",
        "    return 100 * (tf.abs(y_true - y_pred) / ((y_true + tf.abs(y_pred)) / 2))\n",
        "class ModelBasic:\n",
        "    def __init__(self,df_input):\n",
        "        self.df_input = df_input\n",
        "\n",
        "    def modelling(self):\n",
        "        print(f\"modelling...\")\n",
        "        raw_features = self.df_input.loc[\n",
        "            :,\n",
        "            [\n",
        "                \"ticker\",\n",
        "                \"date\",\n",
        "                \"rd_activo_news\",\n",
        "                # Las comento de momento ya que tendremos que replantear el modelo\n",
        "                # y por otro lado, tendremos que evitar lo m√°ximo posible el sesgo\n",
        "                # de look-ahead\n",
        "                # \"r_d_menos_1\",\n",
        "                # \"r_d_mas_1\",\n",
        "                # \"beta\",\n",
        "                # \"r_d_bench\",\n",
        "                # \"r_adj_menos_1\",\n",
        "                # \"r_adj_mas_1\",\n",
        "            ],\n",
        "        ]\n",
        "        raw_features[\"date\"] = pd.to_datetime(raw_features.date, format=\"%Y-%m-%d\")\n",
        "        tickers = raw_features.ticker.unique()\n",
        "\n",
        "        # One-hot encoding\n",
        "        for ticker in tickers[1:]:\n",
        "            raw_features[ticker] = raw_features.ticker == ticker\n",
        "\n",
        "        # Seasonal variations (Fourier series)\n",
        "        dayofyear = raw_features.date.dt.dayofyear\n",
        "        for k in range(1, len(tickers)):\n",
        "            raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
        "            raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
        "            raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
        "                raw_features[f\"sin{k}\"] * raw_features[tickers[k]]\n",
        "            )\n",
        "            raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
        "                raw_features[f\"cos{k}\"] * raw_features[tickers[k]]\n",
        "            )\n",
        "\n",
        "        raw_features = raw_features.drop(columns=\"ticker\")\n",
        "        raw_features = raw_features.drop(columns=\"date\")\n",
        "        # raw_features = pd.get_dummies(raw_features)\n",
        "\n",
        "        features = raw_features.values\n",
        "        # target = df_input.alpha_expost_label\n",
        "        target = df_input.alpha_expost\n",
        "\n",
        "        print(features, target)\n",
        "\n",
        "        x_train, x_test, y_train, y_test = train_test_split(\n",
        "            features, target, test_size=0.33, \n",
        "        )\n",
        "        x_train = np.asarray(x_train).astype(\"float32\")\n",
        "        x_test = np.asarray(x_test).astype(\"float32\")\n",
        "        y_train = np.asarray(y_train).astype(\"float32\")\n",
        "        y_test = np.asarray(y_test).astype(\"float32\")\n",
        "\n",
        "        # revisar los inf\n",
        "        x_train[np.isinf(x_train)] = 0\n",
        "        x_test[np.isinf(x_test)] = 0\n",
        "        y_train[np.isinf(y_train)] = 0\n",
        "        y_test[np.isinf(y_test)] = 0\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        x_train = scaler.fit_transform(x_train)\n",
        "        x_test = scaler.transform(x_test)\n",
        "\n",
        "        # y_train = y_train.reshape(-1, 1)\n",
        "        # y_test = y_test.reshape(-1, 1)\n",
        "\n",
        "        # model = keras.Sequential()\n",
        "        # model.add(keras.layers.Input(shape=(x_train.shape[1],), name=\"entrada\"))\n",
        "        # model.add(keras.layers.Dense(1, name=\"salida\"))\n",
        "        # model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
        "        #           loss='mean_squared_error',\n",
        "        #           metrics=['acc'])\n",
        "\n",
        "        model = keras.Sequential()\n",
        "        model.add(keras.layers.Input(shape=(x_train.shape[1],), name=\"entrada\"))\n",
        "        model.add(\n",
        "            keras.layers.Dense(units=64, kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "        )\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.Activation(\"relu\"))\n",
        "        model.add(tf.keras.layers.Dropout(0.2))\n",
        "        model.add(\n",
        "            keras.layers.Dense(units=32, kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "        )\n",
        "        model.add(tf.keras.layers.BatchNormalization())\n",
        "        model.add(tf.keras.layers.Activation(\"relu\"))\n",
        "        model.add(tf.keras.layers.Dropout(0.2))\n",
        "        model.add(keras.layers.Dense(1, name=\"salida\"))\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "            # loss=smape_loss,\n",
        "            loss=\"mean_squared_error\",\n",
        "            metrics=[\"acc\"],\n",
        "        )\n",
        "\n",
        "        history = model.fit(x_train, y_train, epochs=200, batch_size=x_train.shape[0], validation_split=0.2)\n",
        "\n",
        "        y_pred = model.predict(x_test)\n",
        "        # y_pred = np.exp(model.predict(x_test))\n",
        "\n",
        "        for i in range(10):\n",
        "            print(f'y_test[{i}] : {y_test[i]} , y_pred[{i}] : {y_pred[i]}')\n",
        "\n",
        "        smape = np.mean(smape_loss(y_test, y_pred))        \n",
        "        print(f\"SMAPE: {smape:.5f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Unnamed: 0 ticker                           sector  \\\n",
            "0           36284    TEF  Tecnolog√≠a y Telecomunicaciones   \n",
            "1           23026   AENA             Servicios de Consumo   \n",
            "2           32912   CABK            Servicios Financieros   \n",
            "3            4142   BBVA            Servicios Financieros   \n",
            "4           20395   BBVA            Servicios Financieros   \n",
            "...           ...    ...                              ...   \n",
            "30444       32852    MRL          Servicios Inmobiliarios   \n",
            "30445       20327   BBVA            Servicios Financieros   \n",
            "30446       30344   CABK            Servicios Financieros   \n",
            "30447       15086    SAN            Servicios Financieros   \n",
            "30448       37647   CABK            Servicios Financieros   \n",
            "\n",
            "                        subsector                 date  \\\n",
            "0      Telecomunicaciones y Otros  2021-12-21 14:11:20   \n",
            "1       Transporte y Distribuci√≥n  2021-09-15 10:16:00   \n",
            "2        Bancos y Cajas de Ahorro  2021-05-19 10:07:21   \n",
            "3        Bancos y Cajas de Ahorro  2021-08-03 07:12:05   \n",
            "4        Bancos y Cajas de Ahorro  2021-07-13 17:16:00   \n",
            "...                           ...                  ...   \n",
            "30444                      SOCIMI  2021-12-20 13:12:00   \n",
            "30445    Bancos y Cajas de Ahorro  2021-03-11 09:29:00   \n",
            "30446    Bancos y Cajas de Ahorro  2021-01-20 17:08:00   \n",
            "30447    Bancos y Cajas de Ahorro  2021-06-08 19:01:00   \n",
            "30448    Bancos y Cajas de Ahorro  2021-10-21 09:51:33   \n",
            "\n",
            "                                                   title  \\\n",
            "0      Telef√≥nica y los sindicatos acercan posturas e...   \n",
            "1      S√°nchez invita a ERC a retomar la negociaci√≥n ...   \n",
            "2      Bot√≠n: Orcel incumpli√≥ el compromiso de reduci...   \n",
            "3      Consecuencias individuales y colectivas de los...   \n",
            "4      Niw.es cierra su primer a√±o con m√°s de cinco m...   \n",
            "...                                                  ...   \n",
            "30444  Merlin impulsar√° la eficiencia energ√©tica de s...   \n",
            "30445  BBVA invirti√≥ 142,2 millones en iniciativas so...   \n",
            "30446  Un programa de la Fundaci√≥n 'la Caixa' ofrece ...   \n",
            "30447  Bot√≠n pide a empresarios que sean optimistas y...   \n",
            "30448  Esto es lo que te pueden cobrar por sacar dine...   \n",
            "\n",
            "                                                    body  rd_activo_news  \\\n",
            "0      Telef√≥nica Espa√±a y los sindicatos UGT y CC OO...        0.005175   \n",
            "1      El presidente dice que se cumplen los par√°metr...       -0.011058   \n",
            "2      Las posturas encontradas de Andrea Orcel y Ana...       -0.013114   \n",
            "3      Que la banca despide ‚Äúbien‚Äù, con condiciones r...        0.005489   \n",
            "4      MADRID, 13 Jul. (EUROPA PRESS) -<EOL>Niw.es, l...       -0.021523   \n",
            "...                                                  ...             ...   \n",
            "30444  MADRID, 20 Dic. (EUROPA PRESS) -<EOL>El Banco ...       -0.040911   \n",
            "30445  MADRID, 11 Mar. (EUROPA PRESS) -<EOL>BBVA invi...       -0.012121   \n",
            "30446  PALMA, 20 Ene. (EUROPA PRESS) -<EOL>El Program...       -0.022691   \n",
            "30447  La presidenta de Banco Santander, Ana Bot√≠n, h...        0.000441   \n",
            "30448  La situaci√≥n de bajos tipos de inter√©s ha obli...       -0.025935   \n",
            "\n",
            "       rd_bench_news  alpha_exante  ...  vola20_expost  vola20_diff  \\\n",
            "0           0.005872     -0.000566  ...       0.170079    -0.107451   \n",
            "1          -0.013786     -0.000074  ...       0.251476     0.041173   \n",
            "2          -0.005510      0.004644  ...       0.216964     0.017747   \n",
            "3           0.002018     -0.006552  ...       0.275542    -0.063024   \n",
            "4          -0.014700     -0.001009  ...       0.283612    -0.051412   \n",
            "...              ...           ...  ...            ...          ...   \n",
            "30444       0.015979     -0.012613  ...       0.154901    -0.101482   \n",
            "30445       0.003984      0.013318  ...       0.415522     0.148668   \n",
            "30446      -0.001964     -0.009367  ...       0.353928     0.001223   \n",
            "30447       0.000291     -0.016663  ...       0.243692     0.021261   \n",
            "30448      -0.002010      0.005861  ...       0.255290    -0.069057   \n",
            "\n",
            "       vola20_umbral  vola_label_calc  topic  topic_duda  finanzas_s_n  \\\n",
            "0           0.059450              1.0    NaN         NaN           NaN   \n",
            "1           0.060661              0.0    NaN         NaN           NaN   \n",
            "2           0.082036              0.0    NaN         NaN           NaN   \n",
            "3           0.080959              0.0    NaN         NaN           NaN   \n",
            "4           0.080959              0.0    NaN         NaN           NaN   \n",
            "...              ...              ...    ...         ...           ...   \n",
            "30444       0.059153              1.0    NaN         NaN           NaN   \n",
            "30445       0.080959             -1.0    NaN         NaN           NaN   \n",
            "30446       0.082036              0.0    NaN         NaN           NaN   \n",
            "30447       0.080756              0.0    NaN         NaN           NaN   \n",
            "30448       0.082036              0.0    NaN         NaN           NaN   \n",
            "\n",
            "       finanzas_s_n_duda  impacto_s_n  impacto_s_n_duda  \n",
            "0                    NaN          NaN               NaN  \n",
            "1                    NaN          NaN               NaN  \n",
            "2                    NaN          NaN               NaN  \n",
            "3                    NaN          NaN               NaN  \n",
            "4                    NaN          NaN               NaN  \n",
            "...                  ...          ...               ...  \n",
            "30444                NaN          NaN               NaN  \n",
            "30445                NaN          NaN               NaN  \n",
            "30446                NaN          NaN               NaN  \n",
            "30447                NaN          NaN               NaN  \n",
            "30448                NaN          NaN               NaN  \n",
            "\n",
            "[30449 rows x 27 columns]\n",
            "modelling...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"sin{k}\"] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"cos{k}\"] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_sin{k}\"] = (\n",
            "C:\\Users\\Tienda\\AppData\\Local\\Temp/ipykernel_64644/1060488693.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  raw_features[f\"{tickers[k]}_cos{k}\"] = (\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0051746557900174 False False ... 0.9668478136052826 0.0 0.0]\n",
            " [-0.0110582633067428 True False ... -0.06450844944933187 -0.0 -0.0]\n",
            " [-0.0131137799954521 False True ... -0.47495107206703463 0.0 -0.0]\n",
            " ...\n",
            " [-0.0226914111874445 False True ... 0.8695893893466108 -0.0 0.0]\n",
            " [0.0004412738324759 False False ... 0.02151609743623148 0.0 0.0]\n",
            " [-0.025935264378727 False True ... 0.357698238833147 0.0 0.0]] 0        0.000895\n",
            "1        0.004104\n",
            "2       -0.002486\n",
            "3        0.002747\n",
            "4        0.003817\n",
            "           ...   \n",
            "30444   -0.061265\n",
            "30445   -0.002704\n",
            "30446   -0.028762\n",
            "30447    0.000027\n",
            "30448   -0.016565\n",
            "Name: alpha_expost, Length: 30449, dtype: float64\n",
            "Epoch 1/200\n",
            "1/1 [==============================] - 1s 1s/step - loss: 4.2665 - acc: 0.0000e+00 - val_loss: 1.7971 - val_acc: 0.0000e+00\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 3.0045 - acc: 0.0000e+00 - val_loss: 1.3899 - val_acc: 0.0000e+00\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 2.4238 - acc: 0.0000e+00 - val_loss: 1.3850 - val_acc: 0.0000e+00\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 1.9618 - acc: 0.0000e+00 - val_loss: 1.3643 - val_acc: 0.0000e+00\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 1.6347 - acc: 0.0000e+00 - val_loss: 1.3159 - val_acc: 0.0000e+00\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 1.4032 - acc: 0.0000e+00 - val_loss: 1.2714 - val_acc: 0.0000e+00\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 1.2706 - acc: 0.0000e+00 - val_loss: 1.2276 - val_acc: 0.0000e+00\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 1.2245 - acc: 0.0000e+00 - val_loss: 1.1754 - val_acc: 0.0000e+00\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 1.2034 - acc: 0.0000e+00 - val_loss: 1.1078 - val_acc: 0.0000e+00\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 1.1533 - acc: 0.0000e+00 - val_loss: 1.0288 - val_acc: 0.0000e+00\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 1.0854 - acc: 0.0000e+00 - val_loss: 0.9451 - val_acc: 0.0000e+00\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 1.0102 - acc: 0.0000e+00 - val_loss: 0.8633 - val_acc: 0.0000e+00\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.9577 - acc: 0.0000e+00 - val_loss: 0.7843 - val_acc: 0.0000e+00\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.8811 - acc: 0.0000e+00 - val_loss: 0.7143 - val_acc: 0.0000e+00\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.7992 - acc: 0.0000e+00 - val_loss: 0.6560 - val_acc: 0.0000e+00\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.7265 - acc: 0.0000e+00 - val_loss: 0.6082 - val_acc: 0.0000e+00\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.6724 - acc: 0.0000e+00 - val_loss: 0.5682 - val_acc: 0.0000e+00\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.6350 - acc: 0.0000e+00 - val_loss: 0.5334 - val_acc: 0.0000e+00\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.5865 - acc: 0.0000e+00 - val_loss: 0.5015 - val_acc: 0.0000e+00\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.5450 - acc: 0.0000e+00 - val_loss: 0.4722 - val_acc: 0.0000e+00\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.5159 - acc: 0.0000e+00 - val_loss: 0.4450 - val_acc: 0.0000e+00\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.4801 - acc: 0.0000e+00 - val_loss: 0.4198 - val_acc: 0.0000e+00\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.4513 - acc: 0.0000e+00 - val_loss: 0.3964 - val_acc: 0.0000e+00\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.4238 - acc: 0.0000e+00 - val_loss: 0.3746 - val_acc: 0.0000e+00\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.3981 - acc: 0.0000e+00 - val_loss: 0.3541 - val_acc: 0.0000e+00\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 193ms/step - loss: 0.3767 - acc: 0.0000e+00 - val_loss: 0.3345 - val_acc: 0.0000e+00\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.3526 - acc: 0.0000e+00 - val_loss: 0.3159 - val_acc: 0.0000e+00\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.3310 - acc: 0.0000e+00 - val_loss: 0.2982 - val_acc: 0.0000e+00\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.3123 - acc: 0.0000e+00 - val_loss: 0.2814 - val_acc: 0.0000e+00\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.2916 - acc: 0.0000e+00 - val_loss: 0.2654 - val_acc: 0.0000e+00\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.2744 - acc: 0.0000e+00 - val_loss: 0.2501 - val_acc: 0.0000e+00\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.2579 - acc: 0.0000e+00 - val_loss: 0.2356 - val_acc: 0.0000e+00\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.2435 - acc: 0.0000e+00 - val_loss: 0.2218 - val_acc: 0.0000e+00\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.2268 - acc: 0.0000e+00 - val_loss: 0.2087 - val_acc: 0.0000e+00\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.2130 - acc: 0.0000e+00 - val_loss: 0.1963 - val_acc: 0.0000e+00\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.2005 - acc: 0.0000e+00 - val_loss: 0.1846 - val_acc: 0.0000e+00\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.1886 - acc: 0.0000e+00 - val_loss: 0.1735 - val_acc: 0.0000e+00\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.1774 - acc: 0.0000e+00 - val_loss: 0.1630 - val_acc: 0.0000e+00\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.1656 - acc: 0.0000e+00 - val_loss: 0.1531 - val_acc: 0.0000e+00\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 0.1556 - acc: 0.0000e+00 - val_loss: 0.1438 - val_acc: 0.0000e+00\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 0.1452 - acc: 0.0000e+00 - val_loss: 0.1351 - val_acc: 0.0000e+00\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.1379 - acc: 0.0000e+00 - val_loss: 0.1268 - val_acc: 0.0000e+00\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.1299 - acc: 0.0000e+00 - val_loss: 0.1191 - val_acc: 0.0000e+00\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 0.1208 - acc: 0.0000e+00 - val_loss: 0.1118 - val_acc: 0.0000e+00\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 0.1130 - acc: 0.0000e+00 - val_loss: 0.1049 - val_acc: 0.0000e+00\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.1071 - acc: 0.0000e+00 - val_loss: 0.0984 - val_acc: 0.0000e+00\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.0996 - acc: 0.0000e+00 - val_loss: 0.0922 - val_acc: 0.0000e+00\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.0942 - acc: 0.0000e+00 - val_loss: 0.0865 - val_acc: 0.0000e+00\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.0887 - acc: 0.0000e+00 - val_loss: 0.0810 - val_acc: 0.0000e+00\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.0830 - acc: 0.0000e+00 - val_loss: 0.0759 - val_acc: 0.0000e+00\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.0789 - acc: 0.0000e+00 - val_loss: 0.0711 - val_acc: 0.0000e+00\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.0733 - acc: 0.0000e+00 - val_loss: 0.0666 - val_acc: 0.0000e+00\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 205ms/step - loss: 0.0694 - acc: 0.0000e+00 - val_loss: 0.0625 - val_acc: 0.0000e+00\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.0653 - acc: 0.0000e+00 - val_loss: 0.0587 - val_acc: 0.0000e+00\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.0611 - acc: 0.0000e+00 - val_loss: 0.0551 - val_acc: 0.0000e+00\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.0578 - acc: 0.0000e+00 - val_loss: 0.0517 - val_acc: 0.0000e+00\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.0548 - acc: 0.0000e+00 - val_loss: 0.0486 - val_acc: 0.0000e+00\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.0515 - acc: 0.0000e+00 - val_loss: 0.0457 - val_acc: 0.0000e+00\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.0480 - acc: 0.0000e+00 - val_loss: 0.0430 - val_acc: 0.0000e+00\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 0.0454 - acc: 0.0000e+00 - val_loss: 0.0404 - val_acc: 0.0000e+00\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.0426 - acc: 0.0000e+00 - val_loss: 0.0381 - val_acc: 0.0000e+00\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.0407 - acc: 0.0000e+00 - val_loss: 0.0359 - val_acc: 0.0000e+00\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.0380 - acc: 0.0000e+00 - val_loss: 0.0339 - val_acc: 0.0000e+00\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.0358 - acc: 0.0000e+00 - val_loss: 0.0319 - val_acc: 0.0000e+00\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.0341 - acc: 0.0000e+00 - val_loss: 0.0301 - val_acc: 0.0000e+00\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.0317 - acc: 0.0000e+00 - val_loss: 0.0285 - val_acc: 0.0000e+00\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.0303 - acc: 0.0000e+00 - val_loss: 0.0269 - val_acc: 0.0000e+00\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.0290 - acc: 0.0000e+00 - val_loss: 0.0254 - val_acc: 0.0000e+00\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.0271 - acc: 0.0000e+00 - val_loss: 0.0241 - val_acc: 0.0000e+00\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.0255 - acc: 0.0000e+00 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.0239 - acc: 0.0000e+00 - val_loss: 0.0216 - val_acc: 0.0000e+00\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.0226 - acc: 0.0000e+00 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.0213 - acc: 0.0000e+00 - val_loss: 0.0194 - val_acc: 0.0000e+00\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.0200 - acc: 0.0000e+00 - val_loss: 0.0184 - val_acc: 0.0000e+00\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.0190 - acc: 0.0000e+00 - val_loss: 0.0176 - val_acc: 0.0000e+00\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 232ms/step - loss: 0.0186 - acc: 0.0000e+00 - val_loss: 0.0167 - val_acc: 0.0000e+00\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.0170 - acc: 0.0000e+00 - val_loss: 0.0160 - val_acc: 0.0000e+00\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.0162 - acc: 0.0000e+00 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.0154 - acc: 0.0000e+00 - val_loss: 0.0147 - val_acc: 0.0000e+00\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 0.0146 - acc: 0.0000e+00 - val_loss: 0.0142 - val_acc: 0.0000e+00\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.0139 - acc: 0.0000e+00 - val_loss: 0.0136 - val_acc: 0.0000e+00\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.0131 - val_acc: 0.0000e+00\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.0126 - val_acc: 0.0000e+00\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 0.0121 - acc: 0.0000e+00 - val_loss: 0.0122 - val_acc: 0.0000e+00\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.0117 - val_acc: 0.0000e+00\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.0109 - acc: 0.0000e+00 - val_loss: 0.0113 - val_acc: 0.0000e+00\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.0106 - acc: 0.0000e+00 - val_loss: 0.0109 - val_acc: 0.0000e+00\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.0101 - acc: 0.0000e+00 - val_loss: 0.0105 - val_acc: 0.0000e+00\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.0096 - acc: 0.0000e+00 - val_loss: 0.0101 - val_acc: 0.0000e+00\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.0091 - acc: 0.0000e+00 - val_loss: 0.0098 - val_acc: 0.0000e+00\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 0.0093 - acc: 0.0000e+00 - val_loss: 0.0095 - val_acc: 0.0000e+00\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.0085 - acc: 0.0000e+00 - val_loss: 0.0092 - val_acc: 0.0000e+00\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.0083 - acc: 0.0000e+00 - val_loss: 0.0089 - val_acc: 0.0000e+00\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.0080 - acc: 0.0000e+00 - val_loss: 0.0087 - val_acc: 0.0000e+00\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.0083 - acc: 0.0000e+00 - val_loss: 0.0086 - val_acc: 0.0000e+00\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0085 - val_acc: 0.0000e+00\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.0075 - acc: 0.0000e+00 - val_loss: 0.0084 - val_acc: 0.0000e+00\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.0082 - acc: 0.0000e+00 - val_loss: 0.0083 - val_acc: 0.0000e+00\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0082 - val_acc: 0.0000e+00\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.0075 - acc: 0.0000e+00 - val_loss: 0.0082 - val_acc: 0.0000e+00\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.0082 - val_acc: 0.0000e+00\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.0076 - acc: 0.0000e+00 - val_loss: 0.0082 - val_acc: 0.0000e+00\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.0082 - val_acc: 0.0000e+00\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.0071 - acc: 0.0000e+00 - val_loss: 0.0082 - val_acc: 0.0000e+00\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0082 - val_acc: 0.0000e+00\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0082 - val_acc: 0.0000e+00\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 226ms/step - loss: 0.0073 - acc: 0.0000e+00 - val_loss: 0.0082 - val_acc: 0.0000e+00\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0081 - val_acc: 0.0000e+00\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0081 - val_acc: 0.0000e+00\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0065 - acc: 0.0000e+00 - val_loss: 0.0080 - val_acc: 0.0000e+00\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.0079 - val_acc: 0.0000e+00\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0062 - acc: 0.0000e+00 - val_loss: 0.0078 - val_acc: 0.0000e+00\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0077 - val_acc: 0.0000e+00\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0076 - val_acc: 0.0000e+00\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0075 - val_acc: 0.0000e+00\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0074 - val_acc: 0.0000e+00\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0074 - val_acc: 0.0000e+00\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0073 - val_acc: 0.0000e+00\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0073 - val_acc: 0.0000e+00\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0072 - val_acc: 0.0000e+00\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0071 - val_acc: 0.0000e+00\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0070 - val_acc: 0.0000e+00\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0069 - val_acc: 0.0000e+00\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 136ms/step - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0067 - val_acc: 0.0000e+00\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0066 - val_acc: 0.0000e+00\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0065 - val_acc: 0.0000e+00\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0064 - val_acc: 0.0000e+00\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0063 - val_acc: 0.0000e+00\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0061 - val_acc: 0.0000e+00\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0061 - val_acc: 0.0000e+00\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0060 - val_acc: 0.0000e+00\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.0035 - acc: 0.0000e+00 - val_loss: 0.0059 - val_acc: 0.0000e+00\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0059 - val_acc: 0.0000e+00\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0058 - val_acc: 0.0000e+00\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0057 - val_acc: 0.0000e+00\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0057 - val_acc: 0.0000e+00\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0057 - val_acc: 0.0000e+00\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.0029 - acc: 0.0000e+00 - val_loss: 0.0056 - val_acc: 0.0000e+00\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0055 - val_acc: 0.0000e+00\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 0.0026 - acc: 0.0000e+00 - val_loss: 0.0054 - val_acc: 0.0000e+00\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 199ms/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0053 - val_acc: 0.0000e+00\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0052 - val_acc: 0.0000e+00\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 202ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0051 - val_acc: 0.0000e+00\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0050 - val_acc: 0.0000e+00\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0048 - val_acc: 0.0000e+00\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 0.0017 - acc: 0.0000e+00 - val_loss: 0.0047 - val_acc: 0.0000e+00\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 0.0016 - acc: 0.0000e+00 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.0015 - acc: 0.0000e+00 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.0014 - acc: 0.0000e+00 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.0013 - acc: 0.0000e+00 - val_loss: 0.0042 - val_acc: 0.0000e+00\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 9.8389e-04 - acc: 0.0000e+00 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 9.6465e-04 - acc: 0.0000e+00 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 8.9299e-04 - acc: 0.0000e+00 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 240ms/step - loss: 8.5307e-04 - acc: 0.0000e+00 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 203ms/step - loss: 7.8166e-04 - acc: 0.0000e+00 - val_loss: 0.0035 - val_acc: 0.0000e+00\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 7.2306e-04 - acc: 0.0000e+00 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 7.1637e-04 - acc: 0.0000e+00 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 6.8672e-04 - acc: 0.0000e+00 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 6.7788e-04 - acc: 0.0000e+00 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 204ms/step - loss: 7.1495e-04 - acc: 0.0000e+00 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 7.6603e-04 - acc: 0.0000e+00 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 6.2146e-04 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 8.0991e-04 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 7.8670e-04 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 152ms/step - loss: 7.7644e-04 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 9.4678e-04 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.0207 - acc: 0.0000e+00 - val_loss: 0.0067 - val_acc: 0.0000e+00\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.1143 - acc: 0.0000e+00 - val_loss: 0.0189 - val_acc: 0.0000e+00\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 0.5491 - acc: 0.0000e+00 - val_loss: 0.0382 - val_acc: 0.0000e+00\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 250ms/step - loss: 0.1223 - acc: 0.0000e+00 - val_loss: 0.0737 - val_acc: 0.0000e+00\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 198ms/step - loss: 0.1207 - acc: 0.0000e+00 - val_loss: 0.1169 - val_acc: 0.0000e+00\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 0.1477 - acc: 0.0000e+00 - val_loss: 0.1621 - val_acc: 0.0000e+00\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.1795 - acc: 0.0000e+00 - val_loss: 0.2076 - val_acc: 0.0000e+00\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 0.2221 - acc: 0.0000e+00 - val_loss: 0.2559 - val_acc: 0.0000e+00\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 0.2645 - acc: 0.0000e+00 - val_loss: 0.3080 - val_acc: 0.0000e+00\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 0.3043 - acc: 0.0000e+00 - val_loss: 0.3598 - val_acc: 0.0000e+00\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 282ms/step - loss: 0.3338 - acc: 0.0000e+00 - val_loss: 0.4000 - val_acc: 0.0000e+00\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 0.3571 - acc: 0.0000e+00 - val_loss: 0.4264 - val_acc: 0.0000e+00\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 197ms/step - loss: 0.3743 - acc: 0.0000e+00 - val_loss: 0.4352 - val_acc: 0.0000e+00\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 0.3837 - acc: 0.0000e+00 - val_loss: 0.4299 - val_acc: 0.0000e+00\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.3884 - acc: 0.0000e+00 - val_loss: 0.4155 - val_acc: 0.0000e+00\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.3852 - acc: 0.0000e+00 - val_loss: 0.3967 - val_acc: 0.0000e+00\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 0.3795 - acc: 0.0000e+00 - val_loss: 0.3772 - val_acc: 0.0000e+00\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.3753 - acc: 0.0000e+00 - val_loss: 0.3601 - val_acc: 0.0000e+00\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 0.3623 - acc: 0.0000e+00 - val_loss: 0.3439 - val_acc: 0.0000e+00\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.3443 - acc: 0.0000e+00 - val_loss: 0.3276 - val_acc: 0.0000e+00\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 0.3270 - acc: 0.0000e+00 - val_loss: 0.3106 - val_acc: 0.0000e+00\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.3108 - acc: 0.0000e+00 - val_loss: 0.2933 - val_acc: 0.0000e+00\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 0.2950 - acc: 0.0000e+00 - val_loss: 0.2764 - val_acc: 0.0000e+00\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 0.2782 - acc: 0.0000e+00 - val_loss: 0.2603 - val_acc: 0.0000e+00\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.2629 - acc: 0.0000e+00 - val_loss: 0.2447 - val_acc: 0.0000e+00\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 0.2464 - acc: 0.0000e+00 - val_loss: 0.2300 - val_acc: 0.0000e+00\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 0.2327 - acc: 0.0000e+00 - val_loss: 0.2160 - val_acc: 0.0000e+00\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.2165 - acc: 0.0000e+00 - val_loss: 0.2029 - val_acc: 0.0000e+00\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 153ms/step - loss: 0.2055 - acc: 0.0000e+00 - val_loss: 0.1907 - val_acc: 0.0000e+00\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 0.1921 - acc: 0.0000e+00 - val_loss: 0.1793 - val_acc: 0.0000e+00\n",
            "y_test[0] : 0.022649656981229782 , y_pred[0] : [-0.0221918]\n",
            "y_test[1] : 0.010847295634448528 , y_pred[1] : [0.01097989]\n",
            "y_test[2] : -0.0021304460242390633 , y_pred[2] : [0.07852146]\n",
            "y_test[3] : 0.000632324896287173 , y_pred[3] : [0.00743883]\n",
            "y_test[4] : -0.013361207209527493 , y_pred[4] : [-0.02971251]\n",
            "y_test[5] : -0.0032845756504684687 , y_pred[5] : [0.0153128]\n",
            "y_test[6] : 0.0016428533708676696 , y_pred[6] : [-0.12135649]\n",
            "y_test[7] : -0.011411217041313648 , y_pred[7] : [0.06500015]\n",
            "y_test[8] : -0.005496961064636707 , y_pred[8] : [0.05938188]\n",
            "y_test[9] : 0.013254659250378609 , y_pred[9] : [0.03814831]\n",
            "SMAPE: -800.85645\n"
          ]
        }
      ],
      "source": [
        "df_input = pd.read_csv('gs://tfm_aideas_datasets/dataset.csv', sep=\";\")\n",
        "print(df_input)\n",
        "model = ModelBasic(df_input)\n",
        "model.modelling()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "model_basic.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('myenv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "7dcd78e7f5f0ad0a315c204891a8b52a5a941e827d459c7091e7f1bb656e120f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
